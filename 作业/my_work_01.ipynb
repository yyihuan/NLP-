{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 编程实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 一个基于树规则的语言生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 语法规则 \n",
    "\n",
    "# 接待员\n",
    " # 句式 = 类型1 类型2 。。。 即类型组合\n",
    " # 父类型 = 子类型1 子类型2 。。。 即子类型组合\n",
    " # 类型 = 内容1|内容2 。。。\n",
    "    # 也可称为，句式和父类型是可扩展的，而父类型扩展至无法扩展的子类型后，停止扩展。也可认为，句式是一种__特殊的父类型__\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = null\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\n",
    "\"\"\"\n",
    "\n",
    "# 典型主谓宾\n",
    "simple_grammar = \"\"\"\n",
    "sentence => noun_phrase verb_phrase\n",
    "noun_phrase => Article Adj* noun\n",
    "Adj* => null | Adj Adj*\n",
    "verb_phrase => verb noun_phrase\n",
    "Article =>  一个 | 这个\n",
    "noun =>   女人 |  篮球 | 桌子 | 小猫\n",
    "verb => 看着   |  坐在 |  听着 | 看见\n",
    "Adj =>  蓝色的 | 好看的 | 小小的\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 语法生成器，将人定义的语法规则转换成计算机字典，即预处理\n",
    " # grammar[类型]=[内容1，内容2......]\n",
    "\n",
    "def create_grammar(grammar_str, split='=>', line_split='\\n'): # 默认使用‘=>’分割类型与内容\n",
    "    grammar = {}\n",
    "    for line in grammar_str.split(line_split): # 按行处理\n",
    "        if not line.strip(): continue # 空行跳过，即 not ''.strip() = True\n",
    "        exp, stmt = line.split(split) # 分割为类型和内容\n",
    "        grammar[exp.strip()] = [s.split() for s in stmt.split('|')] # 进一步分割内容，去掉空格，并建立字典\n",
    "    return grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'host': [['寒暄', '报数', '询问', '业务相关', '结尾']], '报数': [['我是', '数字', '号', ',']], '数字': [['单个数字'], ['数字', '单个数字']], '单个数字': [['1'], ['2'], ['3'], ['4'], ['5'], ['6'], ['7'], ['8'], ['9']], '寒暄': [['称谓', '打招呼'], ['打招呼']], '称谓': [['人称', ',']], '人称': [['先生'], ['女士'], ['小朋友']], '打招呼': [['你好'], ['您好']], '询问': [['请问你要'], ['您需要']], '业务相关': [['玩玩', '具体业务']], '玩玩': [['null']], '具体业务': [['喝酒'], ['打牌'], ['打猎'], ['赌博']], '结尾': [['吗？']]} \n",
      "\n",
      "grammar[\"询问\"]= [['请问你要'], ['您需要']]\n"
     ]
    }
   ],
   "source": [
    "# e.g.\n",
    "grammar = create_grammar(host, split='=')\n",
    "print(grammar,'\\n')\n",
    "print('grammar[\"询问\"]=', grammar[\"询问\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 句子生成器\n",
    "\n",
    "def generate(gram, target): # gram，完整的语法规则，包含词库 target，目标父类型\n",
    "    if target not in gram: return target \n",
    "    \"\"\"\n",
    "    从父类型中随机挑选一个成分，通过递归生成器本身\n",
    "    来扩展（成分本身依旧是父类型）或填充（成分是不可扩展的子类型，则挑选其内容）\n",
    "    \"\"\"\n",
    "    expaned = [generate(gram, t) for t in random.choice(gram[target])]\n",
    "    return ''.join([e if e != '/n' else '\\n' for e in expaned if e != 'null'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好我是1号,请问你要打牌吗？'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(gram=grammar, target='host')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'一个篮球坐在这个小小的好看的小猫'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 改写为数据驱动\n",
    "generate(gram=create_grammar(simple_grammar, split='=>'), target='sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 概率函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from functools import reduce\n",
    "from operator import add, mul\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cxjh168/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "filename = '/Users/cxjh168/Downloads/movie_comments.csv'\n",
    "content = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        link name  \\\n",
       "0  1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1  2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2  3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3  4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4  5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                               中二得很    1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(articles[:10])\\nlen(articles)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读出评论内容\n",
    "articles = content['comment'].tolist()\n",
    "'''\n",
    "print(articles[:10])\n",
    "len(articles)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['吴京意淫到了脑残的地步看了恶心想吐', '首映礼看的太恐怖了这个电影不讲道理的完全就是吴京在实现他这个小粉红的英雄梦各种装备轮番上场视物理逻辑于不顾不得不说有钱真好随意胡闹', '吴京的炒作水平不输冯小刚但小刚至少不会用主旋律来炒作吴京让人看了不舒服为了主旋律而主旋律为了煽情而煽情让人觉得他是个大做作大谎言家729更新片子整体不如湄公河行动1整体不够流畅编剧有毒台词尴尬2刻意做作的主旋律煽情显得如此不合时宜而又多余', '凭良心说好看到不像战狼1的续集完虐湄公河行动', '中二得很', '犯我中华者虽远必诛吴京比这句话还要意淫一百倍', '脑子是个好东西希望编剧们都能有', '三星半实打实的7分第一集在爱国主旋律内部做着各种置换与较劲但第二集才真正显露吴京的野心他终于抛弃李忠志了新增外来班底让硬件实力有机会和国际接轨开篇水下长镜头和诸如铁丝网拦截RPG弹头的细节设计都让国产动作片重新封顶在理念上它甚至做到绣春刀2最想做到的那部分', '开篇长镜头惊险大气引人入胜结合了水平不俗的快剪下实打实的真刀真枪让人不禁热血沸腾特别弹簧床架挡炸弹空手接碎玻璃弹匣割喉等帅得飞起就算前半段铺垫节奏散漫主角光环开太大等也不怕作为一个中国人两个小时弥漫着中国强大得不可侵犯的氛围还是让那颗民族自豪心砰砰砰跳个不停', '15100吴京的冷峰在这部里即像成龙又像杰森斯坦森但体制外的同类型电影主角总是代表个人无能的政府需要求助于这些英雄才能解决难题体现的是个人的价值所以主旋律照抄这种模式实际上是有问题的我们以前嘲笑个人英雄主义却没想到捆绑爱国主义的全能战士更加难以下咽']\n"
     ]
    }
   ],
   "source": [
    "# 清理文本内容，去掉标点符号,但保持各个评论独立\n",
    "def token(string):\n",
    "    return re.findall('\\w+', string)\n",
    "\n",
    "articles_clean = [''.join(token(str(a))) for a in articles]\n",
    "print(articles_clean[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载到本地\n",
    "with open('douban_zhanlang.txt', 'w') as f:\n",
    "    for a in articles_clean:\n",
    "        f.write(a + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/kb/zcxf01xs5p7gmw1kss26fp0h0000gn/T/jieba.cache\n",
      "Loading model cost 0.655 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 文本分解为词组，仅分解前100000行数据\n",
    "def cut(string): return list(jieba.cut(string))\n",
    "\n",
    "TOKEN = []\n",
    "for i,line in enumerate((open('douban_zhanlang.txt'))):\n",
    "    if i > 100000: break\n",
    "    TOKEN += cut(line)\n",
    "TOKEN = [str(t) for t in TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词频统计\n",
    "words_count = Counter(TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 某词在全体样本中出现的概率\n",
    "def prob_1(word):\n",
    "    return words_count[word]/len(TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两词一组 两两组合\n",
    "TOKEN_2_GRAM = [''.join(TOKEN[i:i+2]) for i in range(len(TOKEN[:-2]))]\n",
    "words_count_2 = Counter(TOKEN_2_GRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两个词组合后，同时出现在全体样本的概率\n",
    "def prob_2(word1, word2):\n",
    "    if word1 + word2 in words_count_2:\n",
    "        return words_count_2[word1 + word2] / len(TOKEN_2_GRAM)\n",
    "    else:\n",
    "        return 1 / len(TOKEN_2_GRAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ Pro(w_i|w_j) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得最后的语言模型！！！\n",
    "# 实际上就是一个偏差函数\n",
    "def get_probablity(sentence):\n",
    "    words = cut(sentence)\n",
    "    sentence_pro = 1\n",
    "    # 将目标句子相邻两两组合去全体样本中逐一求概率，有点点像冒泡\n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        next_ = words[i+1]\n",
    "        probability = prob_2(word, next_)\n",
    "        sentence_pro *= probability\n",
    "    return sentence_pro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ Pro(sentence) \\approx P(w1|w2)*P(w2|w3)*P(w3|w4)*P(w4|w5)…… $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 结合！一个人工智障"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_grammar = \"\"\"\n",
    "sentence => noun_phrase verb_phrase\n",
    "noun_phrase => Article Adj* noun\n",
    "Adj* => null | Adj Adj*\n",
    "verb_phrase => verb noun_phrase\n",
    "Article =>  一个 | 这个 | 几个 |那个\n",
    "noun => 篮球 | 桌子 | 小猫 | 小明 | 傻子 | 小狗 | 哈士奇\n",
    "verb => 看着   |  坐在 |  听着 | 看见 | 走路 | 拍摄 |走|跳|闻|打\n",
    "Adj =>  蓝色的 | 好看的 | 小小的|美丽的|难吃的|糟糕的|伤心的|高兴的|难以置信的\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sentence = lambda:generate(gram=create_grammar(my_grammar, split='=>'), target='sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['一个蓝色的傻子看见这个傻子', '那个蓝色的好看的小明跳这个小明', '几个好看的糟糕的小猫打那个小狗', '一个小明闻一个桌子', '几个好看的小小的小明听着几个桌子', '一个伤心的小狗打一个蓝色的小狗', '这个难吃的难以置信的小小的高兴的难吃的好看的蓝色的桌子看着几个美丽的桌子', '一个糟糕的伤心的蓝色的难吃的难吃的好看的傻子拍摄这个糟糕的伤心的难吃的小小的哈士奇', '这个小猫看着一个小狗', '一个傻子闻那个难以置信的哈士奇']\n"
     ]
    }
   ],
   "source": [
    "sentences= []\n",
    "for i in range(1000):\n",
    "    sentences.append(a_sentence())\n",
    "print(sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_prob = [(sen,get_probablity(sen)) for sen in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('一个蓝色的傻子看见这个傻子', 4.983556094584832e-36),\n",
       " ('那个蓝色的好看的小明跳这个小明', 9.939919938475226e-45),\n",
       " ('几个好看的糟糕的小猫打那个小狗', 2.602971840003585e-45),\n",
       " ('一个小明闻一个桌子', 1.3585880599037004e-19),\n",
       " ('几个好看的小小的小明听着几个桌子', 4.719674215391115e-46),\n",
       " ('一个伤心的小狗打一个蓝色的小狗', 4.39012236331804e-47),\n",
       " ('这个难吃的难以置信的小小的高兴的难吃的好看的蓝色的桌子看着几个美丽的桌子', 9.129942153233365e-110),\n",
       " ('一个糟糕的伤心的蓝色的难吃的难吃的好看的傻子拍摄这个糟糕的伤心的难吃的小小的哈士奇', 3.546959446978124e-131),\n",
       " ('这个小猫看着一个小狗', 2.095261884184896e-25),\n",
       " ('一个傻子闻那个难以置信的哈士奇', 2.214913819815481e-37)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_prob[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrange = sorted(sentences_prob, key=lambda sentence: sentence[1], reverse=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('一个小明闻一个桌子', 1.3585880599037004e-19),\n",
       " ('那个小明闻几个桌子', 1.3585880599037004e-19),\n",
       " ('这个小明闻几个桌子', 1.3585880599037004e-19),\n",
       " ('一个小明闻这个傻子', 1.3585880599037004e-19),\n",
       " ('这个小明闻那个小狗', 1.3585880599037004e-19),\n",
       " ('一个小明闻那个小狗', 1.3585880599037004e-19),\n",
       " ('那个小明闻那个桌子', 1.3585880599037004e-19),\n",
       " ('一个篮球走这个小猫', 8.381047536739585e-25),\n",
       " ('一个篮球看着一个篮球', 8.381047536739585e-25),\n",
       " ('一个傻子打几个篮球', 6.285785652554688e-25)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrange[:10] # 太智障了。。。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型问题：\n",
    "1. 数据集质量不好，是针对某部电影评论，在评价以外的句子时有很大偏差\n",
    "2. 模型中采用了大量列表生成式，这会占用大量的内存资源。应该改做列表生成器，或是和硬盘交互。\n",
    "3. 模型中存在大量io密集任务，可以多线程处理，提高效率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 理论问答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Can you come up out 3 sceneraies which use AI methods? \n",
    "\n",
    "Ans: Of course, 无人机、 搜索、 专家系统\n",
    "\n",
    "#### 1. How do we use Github; Why do we use Jupyter and Pycharm;\n",
    "\n",
    "Ans: 下载课件和提交作业；jupyter方便实时展示数据，pycharm功能丰富，且可以完成复杂的项目\n",
    "\n",
    "#### 2. What's the Probability Model?\n",
    "\n",
    "Ans: 一个__偏差函数__，判断一个状态偏离整个状态空间中心的程度。\n",
    "\n",
    "#### 3. Can you came up with some sceneraies at which we could use Probability Model?\n",
    "\n",
    "Ans: 现在商用的几乎所有，例如q0所提\n",
    "\n",
    "#### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?\n",
    "\n",
    "Ans: 因为单纯基于语言规则的ai是不可能实现的，记得有很多研究指出这点。具体原因忘了是因为复杂度太高，还是从数学上看，语言这个符号系统就是不完备的。\n",
    "\n",
    "#### 5. What's the Language Model;\n",
    "\n",
    "Ans: 概率模型的一种。它考虑每个词和后词的关联程度，然后统计在整体样本的出现概率。\n",
    "    \n",
    "#### 6. Can you came up with some sceneraies at which we could use Language Model?\n",
    "\n",
    "Ans: 小爱同学！\n",
    "\n",
    "#### 7. What's the 1-gram language model;\n",
    "\n",
    "Ans: 语言模型的一阶近似，它只考虑和最邻近成分的关系\n",
    "\n",
    "#### 8. What's the disadvantages and advantages of 1-gram language model;\n",
    "\n",
    "Ans: 精度太低\n",
    "\n",
    "#### 9. What't the 2-gram models;\n",
    "\n",
    "Ans: $ Pro(sentence)\\approx P(w1|w2w3)*P(w2|w3w4)*P(w3|w4) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
